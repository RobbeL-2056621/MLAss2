{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f90a764a9b43ab5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T09:58:43.506758700Z",
     "start_time": "2024-05-09T09:58:43.411908300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_layers = (100,100,100)\n",
    "img_dimensions = (300,300)\n",
    "data_type = \"synth\"\n",
    "rotating = False\n",
    "flipping = False\n",
    "add_noise = False\n",
    "change_hue = False\n",
    "brightness = False\n",
    "#classifier = \"MLP\"\n",
    "classifier = \"CNN\"\n",
    "data = []\n",
    "test_set_images = []\n",
    "test_set_labels = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c04dfa1f4a4646",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Loading data + Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6dad156ec980f34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T09:58:43.591605300Z",
     "start_time": "2024-05-09T09:58:43.425945500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import skimage as ski\n",
    "import numpy as np\n",
    "import os\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "\n",
    "if data_type == \"unsynth\" and data == []:\n",
    "    data = []\n",
    "    labels = []\n",
    "    images = []\n",
    "    label = 0\n",
    "\n",
    "    for folder in os.listdir(\"Datasets/unsynth\"):\n",
    "        # nb = 0\n",
    "        for file in os.listdir(\"Datasets/unsynth/\" + folder):\n",
    "            # nb += 1\n",
    "            # if nb >= 10:\n",
    "            #     break\n",
    "            img = ski.io.imread(\"Datasets/unsynth/\" + folder + \"/\" + file)\n",
    "            imgResized = resize(img, img_dimensions)\n",
    "            images.append(imgResized)\n",
    "            #prep = imgResized.astype(float)\n",
    "            #prep = imgResized.astype(np.uint8)\n",
    "            labels.append(label)\n",
    "        label+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5ca711d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T09:58:45.202879200Z",
     "start_time": "2024-05-09T09:58:43.436361700Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'Datasets/synth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m images \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     10\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m folder \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasets/synth\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     13\u001b[0m     nb \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasets/synth/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m folder):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'Datasets/synth'"
     ]
    }
   ],
   "source": [
    "import skimage as ski\n",
    "import numpy as np\n",
    "import os\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "\n",
    "if data_type == \"synth\" and data == []:\n",
    "    data = []\n",
    "    labels = []\n",
    "    images = []\n",
    "    label = 0\n",
    "\n",
    "    for folder in os.listdir(\"Datasets/synth\"):\n",
    "        nb = 0\n",
    "        for file in os.listdir(\"Datasets/synth/\" + folder):\n",
    "            if nb >= 200:\n",
    "                break\n",
    "            nb += 1\n",
    "            img = ski.io.imread(\"Datasets/synth/\" + folder + \"/\" + file)\n",
    "            imgResized = resize(img, img_dimensions)\n",
    "            images.append(imgResized)\n",
    "            #prep = imgResized.astype(float)\n",
    "            #prep = imgResized.astype(np.uint8)\n",
    "            labels.append(label)\n",
    "        label+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "520f3a1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T09:58:45.215332800Z",
     "start_time": "2024-05-09T09:58:45.208022100Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from skimage import io\n",
    "down_scaled_image = resize(images[0], (300, 300))\n",
    "io.imshow(down_scaled_image)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38bbdb3",
   "metadata": {},
   "source": [
    "Rotating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d5b7e5f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-09T09:58:45.210119500Z"
    }
   },
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "from skimage.transform import rotate\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy import flipud\n",
    "\n",
    "if rotating:\n",
    "    newImages = []\n",
    "    newLabels = []\n",
    "    rotations = [0, 90, 180, 270]\n",
    "    for i in range(len(images)):\n",
    "        originalImage = images[i]\n",
    "        for rotation in rotations:\n",
    "            rotatedImage = rotate(originalImage, rotation)\n",
    "            newImages.append(rotatedImage)\n",
    "            newLabels.append(labels[i])\n",
    "        #newImages.append(np.ndarray.flatten(flipud(rotatedImage)))\n",
    "        #newLabels.append(labels[i])\n",
    "\n",
    "    images.extend(newImages)\n",
    "    labels.extend(newLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73bc3a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "if flipping:\n",
    "    newImages = []\n",
    "    newLabels = []\n",
    "    for i in range(len(images)):\n",
    "        originalImage = images[i]\n",
    "        newImages.append(flipud(originalImage))\n",
    "        newLabels.append(labels[i])\n",
    "\n",
    "    images.extend(newImages)\n",
    "    labels.extend(newLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231422e5",
   "metadata": {},
   "source": [
    "Brightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f919f2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-09T09:58:45.212736Z"
    }
   },
   "outputs": [],
   "source": [
    "from skimage import exposure\n",
    "if brightness:\n",
    "    newImages = []\n",
    "    newLabels = []\n",
    "    gammas = [0.75, 0.875, 1, 1.125, 1.25]\n",
    "    for i in range(len(images)):\n",
    "        originalImage = images[i]\n",
    "        for gamma in gammas:\n",
    "            brightness_changed_image = exposure.adjust_gamma(originalImage, gamma)\n",
    "            newImages.append(np.ndarray.flatten(brightness_changed_image))\n",
    "            newLabels.append(labels[i])\n",
    "            #newImages.append(np.ndarray.flatten(flipud(rotatedImage)))\n",
    "            #newLabels.append(labels[i])\n",
    "\n",
    "    images.extend(newImages)\n",
    "    labels.extend(newLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186a0a3e",
   "metadata": {},
   "source": [
    "Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf681a90",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-09T09:58:45.218859700Z"
    }
   },
   "outputs": [],
   "source": [
    "if add_noise:\n",
    "    noise = np.random.normal(0, 0.2, (len(images), len(images[0]), len(images[0][0]), len(images[0][0][0])))\n",
    "    noised_images = images + noise\n",
    "    #noised_data = []\n",
    "    #for img in noised_images:\n",
    "    #    img = np.ndarray.flatten(img)\n",
    "    #    noised_data.append(img)\n",
    "\n",
    "    labels.extend(labels)\n",
    "    images.extend(noised_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab81e19e58cf5ed1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "First we make a classifier based on the not augmented data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28d9cd43be5f011b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T10:07:21.086117300Z",
     "start_time": "2024-04-02T10:07:21.064883100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2030"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f19f36d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-09T09:58:45.225860600Z"
    }
   },
   "outputs": [],
   "source": [
    "if classifier == \"MLP\":\n",
    "    for img in images:\n",
    "        data.append(np.ndarray.flatten(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-09T09:58:45.235741900Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "if classifier == \"MLP\":\n",
    "    trainData, testData, trainLabel, testLabel = train_test_split(data, labels, test_size=0.2, random_state=2056621)\n",
    "\n",
    "    clf = MLPClassifier(alpha=1e-5, hidden_layer_sizes=(100, 100, 100), random_state=2056621, max_iter=300, verbose=True)\n",
    "    clf.fit(trainData, trainLabel)\n",
    "\n",
    "    predLabel = clf.predict(testData)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da368c8e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-09T09:58:45.239731200Z"
    }
   },
   "outputs": [],
   "source": [
    "if classifier == \"MLP\":\n",
    "    classification_report(testLabel, predLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6d3232",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-09T09:58:45.241700600Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69aca2c89db0af5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-09T09:58:45.243695500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "\n",
    "if classifier == \"MLP\":\n",
    "    dump(clf, 'a.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5e3bc19f3e6828",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a437b8d",
   "metadata": {},
   "source": [
    "Verslag\n",
    "-beginnen met trainen normale dataset om gewoon te proberen(na alle images naar 1000, 1000 te doen)\n",
    "-slechte resultaten zoals verwacht (slechter dan random guesses)\n",
    "-dan roteren + flippen toegepast -> aantal images maal 8\n",
    "-paste niet meer in RAM -> disk schrijven wat het heel traag maakte\n",
    "-alleen roteren past het wel in RAM + kleinere images (1000, 1000) -> (500, 500)\n",
    "-hidden layer size ook naar (100, 100, 100, 100)\n",
    "-dit gaf redelijke resultaten, max iterations stond nog op 50 wel + warning niet convergeren\n",
    "-na dit weg te laten convergeerde het met wat betere resultaten\n",
    "-200 200 200 200 gaf niet meteen betere resultaten dan 100 100 100 100, accuracy was zelfs omlaag\n",
    "-nu proberen we om aantal dingen te veranderen (changing brightness, hue, noise, and randomly combining all previous methods)\n",
    "\n",
    "-brightness gaf goede resultaten, maar wss overfitten (5x bijna dezelfde images)\n",
    "-normaliseren images doen we nu (delen door 255, helpt overfitten tegen te gaan en sneller convergeren)\n",
    "-Gustavo had gezegd: training op gegenereerde en testen op echte images\n",
    "-noise geeft goede resultaten (we gebruiken maar 100 100)\n",
    "-size images naar (300, 300), zagen er nog altijd herkenbaar uit\n",
    "-trainen op synthetic data als trainset en normale data als testset geeft slechte resultaten (1/4 accuracy = gokken)\n",
    "-ook eerst problemen met RGB(a) wat niet hetzelfde formaat was tussen synthetic & unsynth\n",
    "\n",
    "\n",
    "Volgende stappen:\n",
    "-Trainen op synthetic images\n",
    "-\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f9c6ee",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-09T09:58:45.246797600Z"
    }
   },
   "outputs": [],
   "source": [
    "if data_type == \"synth\" and test_set_images != []:\n",
    "    test_set_images = []\n",
    "    test_set_labels = []\n",
    "    label = 0\n",
    "    for folder in os.listdir(\"Datasets/unsynth\"):\n",
    "        # nb = 0\n",
    "        for file in os.listdir(\"Datasets/unsynth/\" + folder):\n",
    "            # nb += 1\n",
    "            # if nb >= 10:\n",
    "            #     break\n",
    "            img = ski.io.imread(\"Datasets/unsynth/\" + folder + \"/\" + file)\n",
    "            imgResized = resize(img, (300,300))\n",
    "            test_set_images.append(imgResized)\n",
    "            #prep = imgResized.astype(float)\n",
    "            #prep = imgResized.astype(np.uint8)\n",
    "            test_set_labels.append(label)\n",
    "        label+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60338878",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T09:58:45.323536500Z",
     "start_time": "2024-05-09T09:58:45.248810900Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "if data_type == \"synth\":\n",
    "    def RGBA_to_RGB(images):\n",
    "        return np.delete(arr=images, axis=3, obj=3)\n",
    "\n",
    "    images = RGBA_to_RGB(images)\n",
    "    trainData = []\n",
    "    for image in images:\n",
    "        trainData.append(np.ndarray.flatten(image) / 255)\n",
    "    trainLabel = labels\n",
    "    testData = []\n",
    "\n",
    "    for image in test_set_images:\n",
    "        testData.append(np.ndarray.flatten(image) / 255)\n",
    "    testLabel = test_set_labels\n",
    "\n",
    "    lf = MLPClassifier(alpha=1e-5, hidden_layer_sizes=(100, 100, 100), random_state=2056621, max_iter=300, verbose=True)\n",
    "    clf.fit(trainData, trainLabel)\n",
    "\n",
    "    predLabel = clf.predict(testData)\n",
    "\n",
    "    classification_report(testLabel, predLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43470641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\robin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\robin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\robin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\robin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\robin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\robin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "51/51 [==============================] - 126s 2s/step - loss: 1.4629 - accuracy: 0.2913\n",
      "Epoch 2/10\n",
      "51/51 [==============================] - 125s 2s/step - loss: 1.3137 - accuracy: 0.4027\n",
      "Epoch 3/10\n",
      "51/51 [==============================] - 134s 3s/step - loss: 1.1916 - accuracy: 0.4778\n",
      "Epoch 4/10\n",
      "51/51 [==============================] - 138s 3s/step - loss: 1.0551 - accuracy: 0.5733\n",
      "Epoch 5/10\n",
      "51/51 [==============================] - 135s 3s/step - loss: 0.6843 - accuracy: 0.7414\n",
      "Epoch 6/10\n",
      "51/51 [==============================] - 135s 3s/step - loss: 0.2976 - accuracy: 0.9089\n",
      "Epoch 7/10\n",
      "51/51 [==============================] - 135s 3s/step - loss: 0.0703 - accuracy: 0.9828\n",
      "Epoch 8/10\n",
      "51/51 [==============================] - 147s 3s/step - loss: 0.0742 - accuracy: 0.9908\n",
      "Epoch 9/10\n",
      "51/51 [==============================] - 138s 3s/step - loss: 0.2675 - accuracy: 0.9378\n",
      "Epoch 10/10\n",
      "51/51 [==============================] - 137s 3s/step - loss: 0.0679 - accuracy: 0.9914\n",
      "13/13 - 11s - loss: 1.8846 - accuracy: 0.7044 - 11s/epoch - 843ms/step\n",
      "Accuracy:0.7044335007667542\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "if classifier == \"CNN\":\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(100, (3, 3), activation='relu', input_shape=(300, 300, 3)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(100, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(100, (3, 3), activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(hidden_layers[0], activation='relu'))\n",
    "    model.add(layers.Dense(hidden_layers[1], activation='relu'))\n",
    "    model.add(layers.Dense(hidden_layers[2], activation='relu'))\n",
    "    model.add(layers.Dense(4))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    trainData, testData, trainLabel, testLabel = train_test_split(images, labels, test_size=0.2, random_state=2056621)\n",
    "    history = model.fit(np.asarray(trainData), np.asarray(trainLabel), epochs=10)\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(np.asarray(testData), np.asarray(testLabel), verbose=2)\n",
    "    print(\"Accuracy:\" + str(test_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8e2630c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "print(len(images[1015]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c212284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.17      0.23        52\n",
      "           1       0.28      0.29      0.29        51\n",
      "           2       0.16      0.08      0.11        50\n",
      "           3       0.24      0.46      0.31        50\n",
      "\n",
      "    accuracy                           0.25       203\n",
      "   macro avg       0.25      0.25      0.23       203\n",
      "weighted avg       0.25      0.25      0.23       203\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputString = \"              precision    recall  f1-score   support\\n\\n           0       0.32      0.17      0.23        52\\n           1       0.28      0.29      0.29        51\\n           2       0.16      0.08      0.11        50\\n           3       0.24      0.46      0.31        50\\n\\n    accuracy                           0.25       203\\n   macro avg       0.25      0.25      0.23       203\\nweighted avg       0.25      0.25      0.23       203\\n\"\n",
    "parsed_string = inputString.split('\\n')\n",
    "for line in parsed_string:\n",
    "    print(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
